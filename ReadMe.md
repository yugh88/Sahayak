# Sahayak

Sahayak is an empowering application designed to augment the independence of specially-abled individuals by providing key features such as sign language translation, environment detection, and language translation.

---

## Features

1. **Sign Language Translation:**
   - Converts sign language gestures into text or speech to enhance communication.

2. **Environment Detection:**
   - Detects objects and provides descriptive feedback to users about their surroundings.

3. **Language Translation:**
   - Translates text or speech in different languages to assist with multilingual communication.

---

## Technologies Used

- **Frontend:** React.js, HTML, CSS, JavaScript
- **Backend:** Node.js, Express.js
- **AI/ML:** TensorFlow.js, Mediapipe
- **Other Libraries:** react-router-dom, Axios

---

## Folder Structure

```
Sahayak/
├── public/
│   ├── index.html
│   └── ...
├── src/
│   ├── components/
│   │   ├── HomePage.js
│   │   ├── Navbar.js
│   │   ├── SignLanguageTranslator.js
│   │   ├── EnvironmentDetection.js
│   │   ├── LanguageTranslation.js
│   │   └── ...
│   ├── App.js
│   ├── index.js
│   └── styles.css
├── package.json
└── node_modules/ (created after `npm install`)
```

---

## Getting Started

Follow these instructions to set up the project on your local machine:

### Prerequisites

- Node.js (v14 or higher)
- npm or yarn

### Installation

1. Clone the repository:

   ```bash
   git clone https://github.com/yugh88/Sahayak.git
   cd Sahayak
   ```

2. Install dependencies:

   ```bash
   npm install
   ```

3. Start the development server:

   ```bash
   npm start
   ```

   The app should now be running at `http://localhost:3000`.

4. Build for production:

   ```bash
   npm run build
   ```

   The optimized build files will be available in the `build/` directory.

---

## Deployment

To deploy the application, use the `build/` folder generated by the `npm run build` command. You can host it on platforms like:

- **Netlify**
- **Vercel**
- **GitHub Pages**

---

## Usage Instructions

1. Navigate to the application homepage via the navigation bar.
2. Select a feature from the menu:
   - Use the **Sign Language Translator** to convert gestures.
   - Use **Environment Detection** for object recognition.
   - Use **Language Translation** for multilingual support.
3. Follow the on-screen instructions for each feature.

---

## Contributing

We welcome contributions to improve the application. To contribute:

1. Fork the repository.
2. Create a feature branch:
   ```bash
   git checkout -b feature-name
   ```
3. Commit your changes:
   ```bash
   git commit -m "Add new feature"
   ```
4. Push to the branch:
   ```bash
   git push origin feature-name
   ```
5. Submit a pull request.

---

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## Acknowledgements

- Mediapipe for gesture recognition.
- TensorFlow.js for AI/ML capabilities.
- OpenAI for assistance in ideation and problem-solving.

---

## Contact

For any queries or suggestions, please contact:
- **Yugh Juneja**  
GitHub: [yugh88](https://github.com/yugh88)

